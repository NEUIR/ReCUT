#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ç»“æœåˆ†æå·¥å…·
åˆ†ææµ‹è¯•æ•°æ®ï¼Œè®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶è®¡ç®—ä¸åŒç»„åˆ«çš„è¾“å‡ºå¹³å‡é•¿åº¦
"""

import json
import argparse
import sys
from pathlib import Path
from datetime import datetime


def analyze_test_results(input_file, output_file=None, save_output=True):
    """
    åˆ†ææµ‹è¯•æ•°æ®ï¼Œè®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼Œå¹¶è®¡ç®—ä¸åŒç»„åˆ«çš„è¾“å‡ºå¹³å‡é•¿åº¦

    Args:
        input_file: åŒ…å«æµ‹è¯•æ¡ˆä¾‹çš„JSONæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºmetricsçš„JSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        save_output: æ˜¯å¦ä¿å­˜è¾“å‡ºæ–‡ä»¶

    Returns:
        dict: åˆ†æç»“æœæŒ‡æ ‡
    """
    try:
        # è¯»å–è¾“å…¥æ–‡ä»¶
        print(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {input_file}")
        with open(input_file, 'r', encoding='utf-8') as f:
            test_data = json.load(f)

        print(f"æˆåŠŸåŠ è½½ {len(test_data)} ä¸ªæµ‹è¯•æ¡ˆä¾‹")

        # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
        total_cases = len(test_data)
        correct_em = 0
        correct_acc = 0
        correct_f1 = 0
        correct_math_equal = 0
        valid_answers = 0

        # è¾“å‡ºé•¿åº¦ç»Ÿè®¡
        true_outputs = []
        false_outputs = []

        # åˆ†ææ¯ä¸ªæµ‹è¯•æ¡ˆä¾‹
        for idx, case in enumerate(test_data):
            metrics = case.get('Metrics', {})

            # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æœ‰æ•ˆ
            if metrics.get('is_valid_answer', False):
                valid_answers += 1

            # è®¡ç®—æ­£ç¡®æ¡ˆä¾‹æ•°
            if metrics.get('em', 0) == 1:
                correct_em += 1
            if metrics.get('acc', 0) == 1:
                correct_acc += 1
            if metrics.get('f1', 0) == 1.0:
                correct_f1 += 1

            # å¤„ç† math_equal æŒ‡æ ‡
            if metrics.get('math_equal', False):
                correct_math_equal += 1
                # æ”¶é›†math_equal=trueçš„è¾“å‡ºé•¿åº¦
                if 'Output' in case:
                    true_outputs.append(len(case['Output']))
            else:
                # æ”¶é›†math_equal=falseçš„è¾“å‡ºé•¿åº¦
                if 'Output' in case:
                    false_outputs.append(len(case['Output']))

        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        em_score = correct_em / total_cases if total_cases > 0 else 0
        acc_score = correct_acc / total_cases if total_cases > 0 else 0
        f1_score = correct_f1 / total_cases if total_cases > 0 else 0
        math_equal_score = correct_math_equal / total_cases if total_cases > 0 else 0

        # è®¡ç®—å¹³å‡é•¿åº¦
        avg_true = sum(true_outputs) / len(true_outputs) if true_outputs else 0
        avg_false = sum(false_outputs) / len(false_outputs) if false_outputs else 0
        all_outputs = true_outputs + false_outputs
        avg_overall = sum(all_outputs) / len(all_outputs) if all_outputs else 0

        # æ„å»ºmetricsæ•°æ®
        metrics_result = {
            "overall": {
                "em": round(em_score, 4),
                "acc": round(acc_score, 4),
                "f1": round(f1_score, 4),
                "math_equal": round(math_equal_score, 4),
                "num_valid_answer": f"{valid_answers} of {total_cases}",
                "avg_length": {
                    "true_cases": int(avg_true),
                    "false_cases": int(avg_false),
                    "overall": int(avg_overall)
                }
            },
            "statistics": {
                "total_cases": total_cases,
                "correct_em": correct_em,
                "correct_acc": correct_acc,
                "correct_f1": correct_f1,
                "correct_math_equal": correct_math_equal,
                "valid_answers": valid_answers,
                "true_cases_count": len(true_outputs),
                "false_cases_count": len(false_outputs)
            },
            "metadata": {
                "input_file": str(input_file),
                "analysis_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
        }

        # ä¿å­˜metricsæ–‡ä»¶
        if save_output and output_file:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            Path(output_file).parent.mkdir(parents=True, exist_ok=True)

            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(metrics_result, f, indent=4, ensure_ascii=False)
            print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

        # æ‰“å°åˆ†æç»“æœ
        print("\n" + "=" * 50)
        print("ğŸ“Š æµ‹è¯•ç»“æœåˆ†ææŠ¥å‘Š")
        print("=" * 50)
        print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
        print(f"\nğŸ“ˆ æ•´ä½“æŒ‡æ ‡:")
        print(f"  - æ€»æ¡ˆä¾‹æ•°: {total_cases}")
        print(f"  - æœ‰æ•ˆç­”æ¡ˆæ•°: {valid_answers}")
        print(f"  - EMå¾—åˆ†: {em_score:.2%} ({correct_em}/{total_cases})")
        print(f"  - ACCå¾—åˆ†: {acc_score:.2%} ({correct_acc}/{total_cases})")
        print(f"  - F1å¾—åˆ†: {f1_score:.2%} ({correct_f1}/{total_cases})")
        print(f"  - Math Equalå¾—åˆ†: {math_equal_score:.2%} ({correct_math_equal}/{total_cases})")

        print(f"\nğŸ“ è¾“å‡ºé•¿åº¦åˆ†æ:")
        print(f"  - Math Equal = True çš„æ¡ˆä¾‹:")
        print(f"    â€¢ æ•°é‡: {len(true_outputs)}")
        print(f"    â€¢ å¹³å‡é•¿åº¦: {int(avg_true)} å­—ç¬¦")
        print(f"  - Math Equal = False çš„æ¡ˆä¾‹:")
        print(f"    â€¢ æ•°é‡: {len(false_outputs)}")
        print(f"    â€¢ å¹³å‡é•¿åº¦: {int(avg_false)} å­—ç¬¦")
        print(f"  - æ‰€æœ‰æ¡ˆä¾‹å¹³å‡é•¿åº¦: {int(avg_overall)} å­—ç¬¦")

        if len(true_outputs) > 0 and len(false_outputs) > 0:
            length_diff = avg_true - avg_false
            print(f"  - é•¿åº¦å·®å¼‚: {abs(int(length_diff))} å­—ç¬¦ ({'Trueæ›´é•¿' if length_diff > 0 else 'Falseæ›´é•¿'})")

        print("=" * 50)

        return metrics_result

    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {input_file}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ é”™è¯¯: è§£æJSONæ–‡ä»¶å¤±è´¥ - {e}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        sys.exit(1)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='åˆ†ææµ‹è¯•ç»“æœï¼Œè®¡ç®—è¯„ä¼°æŒ‡æ ‡å’Œè¾“å‡ºé•¿åº¦ç»Ÿè®¡',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åˆ†æå¹¶ä¿å­˜ç»“æœ
  %(prog)s --input test_results.json --output metrics.json

  # åªåˆ†æä¸ä¿å­˜ï¼ˆä»…æ˜¾ç¤ºç»“æœï¼‰
  %(prog)s --input test_results.json --no-save

  # ä½¿ç”¨çŸ­å‚æ•°
  %(prog)s -i test.json -o metrics.json

è¾“å‡ºæ ¼å¼:
  ç¨‹åºä¼šè®¡ç®—ä»¥ä¸‹æŒ‡æ ‡:
  - EM (Exact Match) å¾—åˆ†
  - ACC (Accuracy) å¾—åˆ†  
  - F1 å¾—åˆ†
  - Math Equal å¾—åˆ†
  - ä¸åŒç»„åˆ«çš„è¾“å‡ºå¹³å‡é•¿åº¦
        """
    )

    # å¿…éœ€å‚æ•°
    parser.add_argument('-i', '--input',
                        required=True,
                        help='è¾“å…¥çš„æµ‹è¯•ç»“æœJSONæ–‡ä»¶è·¯å¾„')

    # å¯é€‰å‚æ•°
    parser.add_argument('-o', '--output',
                        help='è¾“å‡ºçš„metrics JSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')

    parser.add_argument('--no-save',
                        action='store_true',
                        help='ä¸ä¿å­˜è¾“å‡ºæ–‡ä»¶ï¼Œä»…æ˜¾ç¤ºåˆ†æç»“æœ')

    # å…¶ä»–é€‰é¡¹
    parser.add_argument('-v', '--version',
                        action='version',
                        version='%(prog)s 1.0')

    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()

    # ç¡®å®šæ˜¯å¦ä¿å­˜è¾“å‡º
    save_output = not args.no_save

    # å¦‚æœéœ€è¦ä¿å­˜ä½†æ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œè‡ªåŠ¨ç”Ÿæˆ
    output_file = args.output
    if save_output and not output_file:
        input_path = Path(args.input)
        output_file = str(input_path.parent / f"{input_path.stem}_metrics.json")
        print(f"è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å: {output_file}")

    # æ‰§è¡Œåˆ†æ
    analyze_test_results(args.input, output_file, save_output)

    print("\nâœ… åˆ†æå®Œæˆï¼")


if __name__ == "__main__":
    main()